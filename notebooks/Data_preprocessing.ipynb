{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d005640",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "In this notebook, we build a pipeline to process Amazon product reviews for sentiment analysis and product recommendation. We will:\n",
    "- Clean and preprocess the data\n",
    "- Tokenize and lemmatize the reviews\n",
    "- Remove stopwords\n",
    "- Add sentiment labels\n",
    "- Prepare the data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9616710",
   "metadata": {},
   "source": [
    "### 2. Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries for the project\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Web deployment\n",
    "from flask import Flask, render_template, request\n",
    "\n",
    "# OpenAI or Hugging Face APIs (\n",
    "import openai\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Miscellaneous\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757384c",
   "metadata": {},
   "source": [
    "### 3. Data cleaning functions\n",
    "\n",
    "We define several helper functions to clean and preprocess the reviews data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d060928",
   "metadata": {},
   "source": [
    "##### 3.1 Remove Empty and Duplicate Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b8fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates_empty (df):\n",
    "    \"\"\"\n",
    "    Cleans a DataFrame of Amazon product reviews by:\n",
    "      - Removing rows where either 'reviews.text' or 'reviews.rating' are missing (NaN).\n",
    "      - Removing duplicate reviews for the same product, keeping only the first occurrence of each unique combination of 'asins' and 'reviews.text'.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.dropna(subset=['reviews.text', 'reviews.rating'])\n",
    "    df = df.drop_duplicates(subset=['asins', 'reviews.text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da1946",
   "metadata": {},
   "source": [
    "#### 3.2 Clean reviews text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62981e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes input text by:\n",
    "      - Removing all non-alphanumeric characters (except spaces).\n",
    "      - Replacing multiple spaces with a single space.\n",
    "      - Stripping leading/trailing whitespace and converting text to lowercase.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", '', str(text))\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd5c498",
   "metadata": {},
   "source": [
    "#### 3.3 Clean product names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "    \"\"\"\n",
    "    Cleans the product name by:\n",
    "      - Removing the phrase 'Includes Special Offers'\n",
    "      - Replacing multiple spaces with a single space\n",
    "      - Stripping leading/trailing whitespace\n",
    "    \"\"\"\n",
    "    name = re.sub(r'Includes Special Offers', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    return name.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de95575b",
   "metadata": {},
   "source": [
    "### 4. Reviews Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b393ec",
   "metadata": {},
   "source": [
    "#### 4.1 Combine and Clean Review Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80105f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews(df):\n",
    "    \"\"\"\n",
    "    Preprocesses Amazon review data by:\n",
    "      - Combining 'reviews.title' and 'reviews.text' into a single 'combined_reviews' column.\n",
    "      - Cleaning the combined text using the clean_text function.\n",
    "      - Dropping the original 'reviews.title' and 'reviews.text' columns.\n",
    "      - Removing rows with missing values in 'combined_reviews' or 'reviews.rating'.\n",
    "    \"\"\"\n",
    "    df['combined_reviews'] = df['reviews.title'].fillna('') + ' ' + df['reviews.text'].fillna('')\n",
    "    df['combined_reviews'] = df['combined_reviews'].apply(clean_text)\n",
    "    df = df.drop(['reviews.title', 'reviews.text'], axis=1)\n",
    "    df = df.dropna(subset=['combined_reviews', 'reviews.rating'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fbc612",
   "metadata": {},
   "source": [
    "#### 4.2 Tokenize Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(df, col='combined_reviews', new_col='tokens'):\n",
    "    \"\"\"\n",
    "    Tokenizes the text in a specified column of a DataFrame using NLTK's word_tokenize.\n",
    "    \"\"\"\n",
    "    df[new_col] = df[col].apply(word_tokenize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8394e",
   "metadata": {},
   "source": [
    "#### 4.3 Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd35f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, tokens_col='tokens', new_col='tokens_nostop', language='english'):\n",
    "    \"\"\"\n",
    "    Removes stopwords from tokenized text in a specified column of a DataFrame.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    df[new_col] = df[tokens_col].apply(lambda tokens: [w for w in tokens if w.lower() not in stop_words])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5628e",
   "metadata": {},
   "source": [
    "#### 4.4 Lemmatize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ad958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(df, tokens_col='tokens_nostop', new_col='tokens_lemmatized'):\n",
    "    \"\"\"\n",
    "    Lemmatizes tokens in a specified column of a DataFrame using NLTK's WordNetLemmatizer.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[new_col] = df[tokens_col].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a97b9",
   "metadata": {},
   "source": [
    "### 5. Preprocess product names and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a781f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_lemmatize_names_categories(df):\n",
    "    \"\"\"\n",
    "    Preprocesses and lemmatizes the 'name' and 'categories' columns:\n",
    "    - Fills missing values\n",
    "    - Combines into a new column 'name_category'\n",
    "    - Cleans text (lowercase, removes unwanted characters)\n",
    "    - Tokenizes, removes stopwords, and lemmatizes\n",
    "    - Joins lemmatized tokens into a string in 'name_category_lemmatized'\n",
    "    Returns the DataFrame with new columns.\n",
    "    \"\"\"\n",
    "    # Fill missing values\n",
    "    df['name'] = df['name'].fillna('')\n",
    "    df['categories'] = df['categories'].fillna('')\n",
    "\n",
    "    # Combine name and categories into a single column\n",
    "    df['name_category'] = df['name'] + ' ' + df['categories']\n",
    "\n",
    "    # Remove unwanted text from 'name' \n",
    "    df['name_category'] = df['name_category'].apply(clean_name)\n",
    "\n",
    "    # Tokenize the 'name_category' column\n",
    "    df['name_category_tokens'] = df['name_category_clean'].apply(word_tokenize)\n",
    "\n",
    "    # Remove stopwords from the tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['name_category_tokens_nostop'] = df['name_category_tokens'].apply(\n",
    "        lambda tokens: [token for token in tokens if token.lower() not in stop_words]\n",
    "    )  \n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['name_category_tokens_lemmatized'] = df['name_category_tokens_nostop'].apply(\n",
    "        lambda tokens: [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    )\n",
    "\n",
    "    # Join the lemmatized tokens into a single string\n",
    "    df['name_category_lemmatized'] = df['name_category_tokens_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    return df[['name_category', 'name_category_lemmatized']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e78d32",
   "metadata": {},
   "source": [
    "### 6. Sentiment Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eef77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_column(df):\n",
    "    \"\"\"\n",
    "    Adds a 'sentiment' column to the DataFrame based on the 'reviews.rating' value:\n",
    "      - Ratings >= 4 are labeled as 'positive'\n",
    "      - Ratings == 3 are labeled as 'neutral'\n",
    "      - Ratings < 3 are labeled as 'negative'\n",
    "    \"\"\"\n",
    "    def categorize_rating(rating):\n",
    "        \"\"\"\n",
    "        Categorizes the rating into sentiment labels.\n",
    "        \"\"\"\n",
    "        if rating >= 4:\n",
    "            return 'positive'\n",
    "        elif rating == 3:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'negative'\n",
    "    df['sentiment'] = df['reviews.rating'].apply(categorize_rating)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefaf15",
   "metadata": {},
   "source": [
    "### 7. Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(df):\n",
    "    df = drop_duplicates_empty(df)\n",
    "    df = preprocess_reviews(df)\n",
    "    df = add_sentiment_column(df)\n",
    "    df = tokenize_reviews(df)\n",
    "    df = remove_stopwords(df)\n",
    "    df = lemmatize_tokens(df)\n",
    "    df['lemmatized_str'] = df['tokens_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
