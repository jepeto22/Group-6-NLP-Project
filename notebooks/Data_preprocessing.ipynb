{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0d005640",
      "metadata": {
        "id": "0d005640"
      },
      "source": [
        "### 1. Introduction\n",
        "In this notebook, we build a pipeline to process Amazon product reviews for sentiment analysis and product recommendation. We will:\n",
        "- Clean and preprocess the data\n",
        "- Tokenize and lemmatize the reviews\n",
        "- Remove stopwords\n",
        "- Add sentiment labels\n",
        "- Prepare the data for modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9616710",
      "metadata": {
        "id": "b9616710"
      },
      "source": [
        "### 2. Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dd9e84b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd9e84b7",
        "outputId": "9d163283-6f81-4258-bb7f-481fed704605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary libraries for the project\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Web deployment\n",
        "from flask import Flask, render_template, request\n",
        "\n",
        "# OpenAI or Hugging Face APIs (\n",
        "import openai\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "# Miscellaneous\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3757384c",
      "metadata": {
        "id": "3757384c"
      },
      "source": [
        "### 3. Data cleaning functions\n",
        "\n",
        "We define several helper functions to clean and preprocess the reviews data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d060928",
      "metadata": {
        "id": "4d060928"
      },
      "source": [
        "##### 3.1 Remove Empty and Duplicate Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "47b8fcda",
      "metadata": {
        "id": "47b8fcda"
      },
      "outputs": [],
      "source": [
        "def drop_duplicates_empty (df):\n",
        "    \"\"\"\n",
        "    Cleans a DataFrame of Amazon product reviews by:\n",
        "      - Removing rows where either 'reviews.text' or 'reviews.rating' are missing (NaN).\n",
        "      - Removing duplicate reviews for the same product, keeping only the first occurrence of each unique combination of 'asins' and 'reviews.text'.\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.dropna(subset=['reviews.text', 'reviews.rating'])\n",
        "    df = df.drop_duplicates(subset=['asins', 'reviews.text'])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45da1946",
      "metadata": {
        "id": "45da1946"
      },
      "source": [
        "#### 3.2 Clean reviews text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b62981e5",
      "metadata": {
        "id": "b62981e5"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and standardizes input text by:\n",
        "      - Removing all non-alphanumeric characters (except spaces).\n",
        "      - Replacing multiple spaces with a single space.\n",
        "      - Stripping leading/trailing whitespace and converting text to lowercase.\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", '', str(text))\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip().lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdd5c498",
      "metadata": {
        "id": "cdd5c498"
      },
      "source": [
        "#### 3.3 Clean product names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c738dd04",
      "metadata": {
        "id": "c738dd04"
      },
      "outputs": [],
      "source": [
        "def clean_name(name):\n",
        "    \"\"\"\n",
        "    Cleans the product name by:\n",
        "      - Removing the phrase 'Includes Special Offers'\n",
        "      - Replacing multiple spaces with a single space\n",
        "      - Stripping leading/trailing whitespace\n",
        "    \"\"\"\n",
        "    name = re.sub(r'Includes Special Offers', '', name)\n",
        "    name = re.sub(r'\\s+', ' ', name)\n",
        "    return name.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de95575b",
      "metadata": {
        "id": "de95575b"
      },
      "source": [
        "### 4. Reviews Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29b393ec",
      "metadata": {
        "id": "29b393ec"
      },
      "source": [
        "#### 4.1 Combine and Clean Review Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "80105f13",
      "metadata": {
        "id": "80105f13"
      },
      "outputs": [],
      "source": [
        "def preprocess_reviews(df):\n",
        "    \"\"\"\n",
        "    Preprocesses Amazon review data by:\n",
        "      - Combining 'reviews.title' and 'reviews.text' into a single 'combined_reviews' column.\n",
        "      - Cleaning the combined text using the clean_text function.\n",
        "      - Dropping the original 'reviews.title' and 'reviews.text' columns.\n",
        "      - Removing rows with missing values in 'combined_reviews' or 'reviews.rating'.\n",
        "    \"\"\"\n",
        "    df['combined_reviews'] = df['reviews.title'].fillna('') + ' ' + df['reviews.text'].fillna('')\n",
        "    df['combined_reviews'] = df['combined_reviews'].apply(clean_text)\n",
        "    df = df.drop(['reviews.title', 'reviews.text'], axis=1)\n",
        "    df = df.dropna(subset=['combined_reviews', 'reviews.rating'])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5fbc612",
      "metadata": {
        "id": "b5fbc612"
      },
      "source": [
        "#### 4.2 Tokenize Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "04b6da1a",
      "metadata": {
        "id": "04b6da1a"
      },
      "outputs": [],
      "source": [
        "def tokenize_reviews(df, col='combined_reviews', new_col='tokens'):\n",
        "    \"\"\"\n",
        "    Tokenizes the text in a specified column of a DataFrame using NLTK's word_tokenize.\n",
        "    \"\"\"\n",
        "    df[new_col] = df[col].apply(word_tokenize)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f8394e",
      "metadata": {
        "id": "94f8394e"
      },
      "source": [
        "#### 4.3 Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ecd35f36",
      "metadata": {
        "id": "ecd35f36"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(df, tokens_col='tokens', new_col='tokens_nostop', language='english'):\n",
        "    \"\"\"\n",
        "    Removes stopwords from tokenized text in a specified column of a DataFrame.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words(language))\n",
        "    df[new_col] = df[tokens_col].apply(lambda tokens: [w for w in tokens if w.lower() not in stop_words])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab5628e",
      "metadata": {
        "id": "3ab5628e"
      },
      "source": [
        "#### 4.4 Lemmatize tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fd1ad958",
      "metadata": {
        "id": "fd1ad958"
      },
      "outputs": [],
      "source": [
        "def lemmatize_tokens(df, tokens_col='tokens_nostop', new_col='tokens_lemmatized'):\n",
        "    \"\"\"\n",
        "    Lemmatizes tokens in a specified column of a DataFrame using NLTK's WordNetLemmatizer.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    df[new_col] = df[tokens_col].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df1a97b9",
      "metadata": {
        "id": "df1a97b9"
      },
      "source": [
        "### 5. Preprocess product names and categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5a781f09",
      "metadata": {
        "id": "5a781f09"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_lemmatize_names_categories(df):\n",
        "    \"\"\"\n",
        "    Preprocesses and lemmatizes the 'name' and 'categories' columns:\n",
        "    - Fills missing values\n",
        "    - Combines into a new column 'name_category'\n",
        "    - Cleans text (lowercase, removes unwanted characters)\n",
        "    - Tokenizes, removes stopwords, and lemmatizes\n",
        "    - Joins lemmatized tokens into a string in 'name_category_lemmatized'\n",
        "    Returns the DataFrame with new columns.\n",
        "    \"\"\"\n",
        "    df['name'] = df['name'].fillna('')\n",
        "    df['categories'] = df['categories'].fillna('')\n",
        "    df['name_category'] = df['name'] + ' ' + df['categories']\n",
        "    df['name_category'] = df['name_category'].apply(clean_name)\n",
        "    df['name_category_tokens'] = df['name_category'].apply(word_tokenize)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    df['name_category_tokens_nostop'] = df['name_category_tokens'].apply(\n",
        "        lambda tokens: [token for token in tokens if token.lower() not in stop_words]\n",
        "    )\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    df['name_category_tokens_lemmatized'] = df['name_category_tokens_nostop'].apply(\n",
        "        lambda tokens: [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    )\n",
        "\n",
        "    df['name_category_lemmatized'] = df['name_category_tokens_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e78d32",
      "metadata": {
        "id": "62e78d32"
      },
      "source": [
        "### 6. Sentiment Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "36eef77d",
      "metadata": {
        "id": "36eef77d"
      },
      "outputs": [],
      "source": [
        "def add_sentiment_column(df):\n",
        "    \"\"\"\n",
        "    Adds a 'sentiment' column to the DataFrame based on the 'reviews.rating' value:\n",
        "      - Ratings >= 4 are labeled as 'positive'\n",
        "      - Ratings == 3 are labeled as 'neutral'\n",
        "      - Ratings < 3 are labeled as 'negative'\n",
        "    \"\"\"\n",
        "    def categorize_rating(rating):\n",
        "        \"\"\"\n",
        "        Categorizes the rating into sentiment labels.\n",
        "        \"\"\"\n",
        "        if rating >= 4:\n",
        "            return 'positive'\n",
        "        elif rating == 3:\n",
        "            return 'neutral'\n",
        "        else:\n",
        "            return 'negative'\n",
        "    df['sentiment'] = df['reviews.rating'].apply(categorize_rating)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eefaf15",
      "metadata": {
        "id": "8eefaf15"
      },
      "source": [
        "### 7. Preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ea31d0ad",
      "metadata": {
        "id": "ea31d0ad"
      },
      "outputs": [],
      "source": [
        "def preprocess_pipeline(df):\n",
        "    df = drop_duplicates_empty(df)\n",
        "    df = preprocess_reviews(df)\n",
        "    df = add_sentiment_column(df)\n",
        "    df = tokenize_reviews(df)\n",
        "    df = remove_stopwords(df)\n",
        "    df = lemmatize_tokens(df)\n",
        "    df['lemmatized_str'] = df['tokens_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
        "    df = preprocess_and_lemmatize_names_categories(df)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('combined_reviews.csv')"
      ],
      "metadata": {
        "id": "funJYlEjsm08"
      },
      "id": "funJYlEjsm08",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = preprocess_pipeline(df)"
      ],
      "metadata": {
        "id": "wfQu6eNBssRW"
      },
      "id": "wfQu6eNBssRW",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('pre_processed_data.csv', index=False)"
      ],
      "metadata": {
        "id": "YcrtvLVdtHjB"
      },
      "id": "YcrtvLVdtHjB",
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}