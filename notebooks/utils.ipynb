{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d005640",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "This section imports the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. Import Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Preprocessing Functions ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d060928",
   "metadata": {},
   "source": [
    "## Drop duplicates empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b8fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates_empty (df):\n",
    "    df = df.dropna(subset=['reviews.text', 'reviews.rating'])\n",
    "    df = df.drop_duplicates(subset=['asins', 'reviews.text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da1946",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62981e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", '', str(text))\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de95575b",
   "metadata": {},
   "source": [
    "## Preprocess reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80105f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews(df):\n",
    "    # Combine title and text, clean, drop NA\n",
    "    df['combined_reviews'] = df['reviews.title'].fillna('') + ' ' + df['reviews.text'].fillna('')\n",
    "    df['combined_reviews'] = df['combined_reviews'].apply(clean_text)\n",
    "    df = df.drop(['reviews.title', 'reviews.text'], axis=1)\n",
    "    df = df.dropna(subset=['combined_reviews', 'reviews.rating'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fbc612",
   "metadata": {},
   "source": [
    "## Tokenize reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(df, col='combined_reviews', new_col='tokens'):\n",
    "    df[new_col] = df[col].apply(word_tokenize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8394e",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd35f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, tokens_col='tokens', new_col='tokens_nostop', language='english'):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    df[new_col] = df[tokens_col].apply(lambda tokens: [w for w in tokens if w.lower() not in stop_words])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5628e",
   "metadata": {},
   "source": [
    "## Lemmatize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ad958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(df, tokens_col='tokens_nostop', new_col='tokens_lemmatized'):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[new_col] = df[tokens_col].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e78d32",
   "metadata": {},
   "source": [
    "## Add sentiment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eef77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_column(df):\n",
    "    def categorize_rating(rating):\n",
    "        if rating >= 4:\n",
    "            return 'positive'\n",
    "        elif rating == 3:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'negative'\n",
    "    df['sentiment'] = df['reviews.rating'].apply(categorize_rating)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61989d3b",
   "metadata": {},
   "source": [
    "## Preprocess and lemmatize names categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13faa8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_lemmatize_names_categories(df):\n",
    "    \"\"\"\n",
    "    Preprocesses and lemmatizes the 'name' and 'categories' columns:\n",
    "    - Fills missing values\n",
    "    - Combines into a new column 'name_category'\n",
    "    - Cleans text (lowercase, removes unwanted characters)\n",
    "    - Tokenizes, removes stopwords, and lemmatizes\n",
    "    - Joins lemmatized tokens into a string in 'name_category_lemmatized'\n",
    "    Returns the DataFrame with new columns.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df82431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "    df['name'] = df['name'].fillna('')\n",
    "    df['name'] = df['name'].str.replace('Includes Special Offers', '', regex=False).str.strip()\n",
    "    df['categories'] = df['categories'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into a single column\n",
    "    df['name_category'] = df['name'] + ' ' + df['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6badd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text: lowercase, remove unwanted characters, strip\n",
    "    def clean_text(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    df['name_category_clean'] = df['name_category'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292fdf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "    df['name_category_tokens'] = df['name_category_clean'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['name_category_nostop'] = df['name_category_tokens'].apply(\n",
    "        lambda tokens: [word for word in tokens if word not in stop_words]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc291a4e",
   "metadata": {},
   "source": [
    "### Text Lemmatization\n",
    "We normalize the review text using lemmatization to reduce words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['name_category_lemmatized'] = df['name_category_nostop'].apply(\n",
    "        lambda tokens: [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff3951",
   "metadata": {},
   "source": [
    "### Text Lemmatization\n",
    "We normalize the review text using lemmatization to reduce words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeaff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join lemmatized tokens into a string\n",
    "    df['name_category_lemmatized'] = df['name_category_lemmatized'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefaf15",
   "metadata": {},
   "source": [
    "## Preprocess pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d5230",
   "metadata": {},
   "source": [
    "### Remove Duplicate or Empty Entries\n",
    "We clean the dataset by dropping irrelevant or duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(df):\n",
    "    df = drop_duplicates_empty(df)\n",
    "    df = preprocess_reviews(df)\n",
    "    df = add_sentiment_column(df)\n",
    "    df = tokenize_reviews(df)\n",
    "    df = remove_stopwords(df)\n",
    "    df = lemmatize_tokens(df)\n",
    "    df['lemmatized_str'] = df['tokens_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
